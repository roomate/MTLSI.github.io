<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="All-At-Once generative modeling in operator space with Stochastic Interpolants, for posterior sampling and inverse problems.">
    <meta name="keywords" content="Stochastic Interpolants, Generative modeling, Posterior Sampling">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Embedd LateX dans ce fichier HTML-->
    <!-- <script defer src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-chtml.js"></script> -->
    <!-- KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
        integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
        integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false}
      ]
    });"></script>

    <title>Multitask Learning with Stochastic Interpolants</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <!--Add an icon on the navigator tab-->
    <link rel="icon" href="./static/images/cat.PNG">

    <!-- Import CSS files -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">


    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Multitask Learning with Stochastic Interpolants</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Hugo Negrel<sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://florentincdx.github.io/" target="_blank">Florentin
                                    Coeurdoux</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="http://malbergo.me/" target="_blank">Michael Albergo</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://wp.nyu.edu/courantinstituteofmathematicalsciences-eve2/"
                                    target="_blank">Eric
                                    Vanden-Eijnden</a><sup>1, 3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Capital Fund Management</span>
                            <span class="author-block"><sup>2</sup>Society of Fellows, Harvard University</span>
                            <span class="author-block"><sup>3</sup>Courant Institute of Mathematical Sciences, New York
                                University, New York</span>
                        </div>

                        <div class="has-text-centered" style="margin-top: 1.5em;">
                            <span class="tag is-info is-medium">Spotlight NeurIPS 2025</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://www.arxiv.org/pdf/2508.04605"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://www.arxiv.org/abs/2508.04605"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Soon</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- TL;DR. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p>
                            <strong>TL;DR</strong> This paper generalizes and extends stochastic interpolants formalism
                            to solve a very large variety of tasks with <b>one single
                                trained model</b>. It provides zero-shot versatility; experimental proofs include exact
                            posterior sampling, inpainting and highly-constrained planning on high-dimensional datasets
                            such as
                            CelebA and $\varphi^4$.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ TL;DR. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop" , style="margin-bottom: 100px;">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            We propose a framework for learning maps between probability distributions that
                            broadly generalizes the time dynamics of flow and diffusion models.
                        </p>
                        <p>
                            To enable this, we generalize stochastic interpolants by replacing the scalar time variable
                            with
                            vectors, matrices, or linear operators, allowing us to bridge probability distributions
                            across multiple dimensional spaces.
                        </p>
                        <p>
                            This approach enables the construction of versatile generative models capable of
                            fulfilling multiple tasks without task-specific
                            training. Our operator-based interpolants not only provide a unifying theoretical
                            perspective for existing generative models but also extend their capabilities.
                            Through numerical experiments, we demonstrate the zero-shot efficacy of our
                            method on conditional generation and inpainting, fine-tuning and posterior sampling
                            and multiscale modeling, suggesting its potential as a generic task-agnostic
                            alternative to specialized models.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>

        <div class="container is-max-desktop" , style="margin-bottom: 100px;">
            <!-- Key Contributions. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Key Contributions</h2>
                    <div class="content has-text-justified">
                        <p>
                            <b>Generalized stochastic interpolants:</b> We replace scalar time with linear operators to
                            interpolate between distributions across dimensions. This points toward a more general
                            paradigm of universal generative models that can be trained once and then applied to a
                            variety of objectives.
                        </p>
                        <p>
                            <b>Unified Generative framework:</b> We provide a single formalism for multitask generation
                            with flows and
                            diffusions, without any sort of retraining.
                        </p>
                        <p>
                            <b>zero-shot versatility:</b> Perform conditional generation, inpainting, fine-tuning, and
                            posterior sampling with one single model. You can impose hard constraints on the data
                            generation,
                            or make multiple generation in a row, one after another.
                        </p>
                        <p>
                            <b>Universal generative models:</b> Amortize training across tasks for scalable,
                            task-agnostic generation. Inpainting and conditional generation can be performed during the
                            same diffusion, saving time and computing resources.
                        </p>
                    </div>
                    <h2 class="title is-3">Presentation Video</h2>
                    <div class="imgContainer">
                        <video poster="" id="steve" autoplay controls muted loop playsinline height="100%" width="100%">
                            <source src="./static/videos/output.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <!--/ Key Contributions. -->

            </div>
        </div>

        <hr style="border:1px solid black; background: black; border: none; margin-left: 350px; margin-right: 350px">

        <!-- Inpainting. -->
        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <h1 class="title is-2 has-text-centered"> Results</h1>
                <h2 class="title is-4 has-text-centered">Inpainting</h2>
                <p class="has-text-justified">
                    Consider images from high-dimensional image datasets CelebA and AFHQ-Cat.
                    With multitask stochastic interpolants, pixel-wise inpainting becomes a trivial, one-shot task.
                    Here, we consider block and random masking, probing two radically different inpainting
                    situation.
                </p>
                <div class="has-text-centered"><b>Dataset: AFHQ-Cat</b></div>
                <div class="container has-text-centered">
                    <div class="imgContainer" , display: flex;>
                        <img src="static/images/0089.png" height="52%" width="52%" ,
                            style="position: relative; transform: translateY(12px);">
                        <p style="margin: 22px 0px 0px 0px;">Original image</p>
                    </div>

                    <div class="imgContainer">
                        <video poster="" id="steve" autoplay controls muted loop playsinline height="50%" width="50%">
                            <source src="./static/videos/block_0089.mp4" type="video/mp4">
                        </video>
                        <p>Block masking</p>
                    </div>
                    <div class="imgContainer">
                        <video poster="" id="steve" autoplay controls muted loop playsinline height="50%" width="50%">
                            <source src="./static/videos/rand_0089.mp4" type="video/mp4">
                        </video>
                        <p>Random masking</p>
                    </div>
                </div>
                <hr
                    style="border:1px solid black; background: black; border: none; margin-left: 380px; margin-right: 380px;">
                <div class="has-text-centered"><b>Dataset: CelebA</b></div>
                <div class="container has-text-centered" , style="margin-bottom: 20px">
                    <div class="imgContainer">
                        <img src="static/images/000034.jpg" height="50%" width="50%" ,
                            style="position: relative; transform: translateY(12px);">
                        <p style="margin: 22px 0px 0px 0px;">Original image</p>
                    </div>

                    <div class="imgContainer">
                        <video poster="" id="steve" autoplay controls muted loop playsinline height="50%" width="50%">
                            <source src="./static/videos/block_000034.mp4" type="video/mp4">
                        </video>
                        <p>Block masking</p>
                    </div>
                    <div class="imgContainer">
                        <video poster="" id="steve" autoplay controls muted loop playsinline height="50%" width="50%">
                            <source src="./static/videos/rand_000034.mp4" type="video/mp4">
                        </video>
                        <p>Random masking</p>
                    </div>
                </div>
                <!--/Inpainting. -->
            </div>
        </div>
        <div class="columns is-centered is-max-desktop has-text-centered" style="margin-bottom: 20px">
            <div class="imgContainer column is-four-fifths" style="justify-content: center">
                <p>PSNR and SSIM metrics on CelebA and AFHQ-Cat datasets. A single generation was performed on each
                    image.</p>
                <table class="table" , style="width: 100%; text-align: center;">
                    <thead>
                        <tr>
                            <th rowspan="3">Method</th>
                            <th colspan="4">CelebA</th>
                            <th colspan="4">AFHQ-Cat</th>
                        </tr>
                        <tr>
                            <th colspan="2">Random</th>
                            <th colspan="2">Block</th>
                            <th colspan="2">Random</th>
                            <th colspan="2">Block</th>
                        </tr>
                        <tr>
                            <th>PSNR</th>
                            <th>SSIM</th>
                            <th>PSNR</th>
                            <th>SSIM</th>
                            <th>PSNR</th>
                            <th>SSIM</th>
                            <th>PSNR</th>
                            <th>SSIM</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Degraded</td>
                            <td>11.82</td>
                            <td>0.197</td>
                            <td>22.12</td>
                            <td>0.742</td>
                            <td>13.35</td>
                            <td>0.234</td>
                            <td>21.50</td>
                            <td>0.744</td>
                        </tr>
                        <tr>
                            <td><cite>pokle2024trainingfree</cite></td>
                            <td>28.36</td>
                            <td>0.865</td>
                            <td>28.84</td>
                            <td>0.914</td>
                            <td>28.84</td>
                            <td>0.838</td>
                            <td>23.88</td>
                            <td>0.874</td>
                        </tr>
                        <tr>
                            <td><cite>hamu2024dflow</cite></td>
                            <td>33.07</td>
                            <td>0.938</td>
                            <td>29.70</td>
                            <td>0.893</td>
                            <td>31.37</td>
                            <td>0.888</td>
                            <td>26.69</td>
                            <td>0.833</td>
                        </tr>
                        <tr>
                            <td><cite>zhang2024flow</cite></td>
                            <td>32.33</td>
                            <td>0.945</td>
                            <td>29.40</td>
                            <td>0.858</td>
                            <td>31.76</td>
                            <td>0.909</td>
                            <td>25.85</td>
                            <td>0.822</td>
                        </tr>
                        <tr>
                            <td><cite>martin2024pnp</cite></td>
                            <td>33.54</td>
                            <td>0.953</td>
                            <td><strong>30.59</strong></td>
                            <td><strong>0.943</strong></td>
                            <td>32.98</td>
                            <td>0.930</td>
                            <td>26.87</td>
                            <td>0.904</td>
                        </tr>
                        <tr>
                            <td>Ours</td>
                            <td><strong>33.76</strong></td>
                            <td><strong>0.967</strong></td>
                            <td>29.98</td>
                            <td>0.938</td>
                            <td><strong>33.11</strong></td>
                            <td><strong>0.945</strong></td>
                            <td><strong>26.96</strong></td>
                            <td><strong>0.914</strong></td>
                        </tr>
                    </tbody>
                </table>

                <p class="column has-text-justified">Of all the inpainting benchmarks, multitask stochastic
                    interpolants perform best while offering great flexibility, both on PSNR and SSIM metrics. The
                    very
                    same model is employed for both block and random inpainting.</p>
            </div>
        </div>
        <div class="columns is-centered">
            <div class=" column is-four-fifths">
                <p class="has-text-justified"> You can perform multiple generations in a row on the
                    same image without significantly deviating from the original image, see below. The eyes and
                    mouth
                    are masked then generated in turn.
                </p>
                <div class="container has-text-centered" , style="margin: 0px 0px 0 0px">
                    <div class="imgContainer">
                        <img src="static/images/original_cat.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px; position: relative; transform: translateY(12px);">
                        <p style="margin: 5px 0px 0px 0px;">Original image</p>
                    </div>
                    <div class="imgContainer">
                        <img src="static/images/cat_eyes_masked.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px; position: relative; transform: translateY(12px);">
                        <p style="margin: 5px 0px 0px 0px;">Masked eyes</p>
                    </div>
                    <div class="imgContainer">
                        <img src="static/images/cat_eyes_generated.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px; position: relative; transform: translateY(12px);">
                        <p style="margin: 5px 0px 0px 0px;">Generated eyes</p>
                    </div>
                    <div class="imgContainer" , style="position: relative; transform: translateY(18px);">
                        <img src="static/images/cat_mouth_masked.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px;">
                        <p style="margin: -5px 0px 0px 0px;">Eye generated + Masked mouth</p>
                    </div>
                    <div class="imgContainer" , style="position: relative; transform: translateY(18px);">
                        <img src="static/images/cat_mouth_generated.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px;">
                        <p style="margin: -5px 0px 0px 0px;">Eye generated + Mouth generated</p>
                    </div>
                </div>
            </div>
        </div>
        <div class="columns is-centered">
            <div class=" column is-four-fifths">
                <div class="container has-text-centered" , style="margin: 0px 0px 0px 0px">
                    <div class="imgContainer">
                        <img src="static/images/000034.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px; position: relative; transform: translateY(12px);">
                        <p style="margin: 5px 0px 0px 0px;">Original image</p>
                    </div>
                    <div class="imgContainer">
                        <img src="static/images/girl_eye_masked.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px; position: relative; transform: translateY(12px);">
                        <p style="margin: 5px 0px 0px 0px;">Masked eyes</p>
                    </div>
                    <div class="imgContainer">
                        <img src="static/images/girl_eye_generated.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px; position: relative; transform: translateY(12px);">
                        <p style="margin: 5px 0px 0px 0px;">Generated eyes</p>
                    </div>
                    <div class="imgContainer" , style="position: relative; transform: translateY(18px);">
                        <img src="static/images/girl_mouth_masked.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px;">
                        <p style="margin: -5px 0px 0px 0px;">Eye generated + Masked mouth</p>
                    </div>
                    <div class="imgContainer" , style="position: relative; transform: translateY(18px);">
                        <img src="static/images/girl_mouth_generated.jpg" height="84%" width="84%" ,
                            style="margin: 30px 0px 0px 0px;">
                        <p style="margin: -5px 0px 0px 0px;">Eye generated + Mouth generated</p>
                    </div>
                </div>

            </div>
        </div>
        <div class="arrow-container" , style="margin-bottom: 40px">
            <div class="arrow"></div>
            <div class="text">Generation flow</div>
        </div>

        <!-- Constrained planning. -->
        <div class="columns is-centered is-max-desktop">
            <div class="column is-four-fifths has-text-justified">
                <h2 class="title is-4 has-text-centered">Constrained planning</h2>
                <p>This problem takes root in Reinforcement Learning (RL). Take a maze and two end
                    points.
                    The
                    task
                    is to find the shortest path joining them
                    while abiding by the contraints imposed in the environment. With stochastic
                    interpolants, to
                    generate a path, you simply fix the first and last point, and eventually let
                    diffuse all
                    other intermediates points.
                </p>
                <div class="container">
                    <video controls autoplay controls muted loop playsinline height="30%" width="30%">
                        <source src="./static/videos/path3.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay controls muted loop playsinline height="30%" width="30%">
                        <source src="./static/videos/path2.mp4" type="video/mp4">
                    </video>
                    <video controls autoplay controls muted loop playsinline height="30%" width="30%">
                        <source src="./static/videos/path1.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    The constraints do not necessarily have to apply only on the end points, you can
                    also
                    impose
                    hard constraints on the pathway, assuming it remains feasible. A small white dot
                    represents
                    the constraints applied on the path, forcing it to take a detour. The path
                    length adapts
                    accordingly.
                </p>
                <div class="container">
                    <div class="imgContainer has-text-centered">
                        <video controls autoplay controls muted loop playsinline height="60%" width="60%">
                            <source src="./static/videos/path_nc.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="imgContainer has-text-centered">
                        <video controls autoplay controls muted loop playsinline height="60%" width="60%">
                            <source src="./static/videos/path_c.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <hr
                    style="border:1px solid black; background: black; border: none; margin-left: 350px; margin-right: 350px; margin-bottom:-10px; margin-top: -10px;">

                <div class="columns">
                    <div class="imgContainer has-text-centered">
                        <video controls autoplay controls muted loop playsinline height="60%" width="60%">
                            <source src="./static/videos/path2_nc.mp4" type="video/mp4">
                        </video>
                        <div class="has-text-centered">
                            <p>Shortest path, no constraint except end-points.</p>
                        </div>
                    </div>
                    <div class="imgContainer has-text-centered">
                        <video controls autoplay controls muted loop playsinline height="60%" width="60%">
                            <source src="./static/videos/path2_c.mp4" type="video/mp4">
                        </video>
                        <p>Path under a constraint at mid-length, here on the bottom-right corner.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- Constrained planning. -->

        <!-- Posterior sampling. -->
        <div class="columns is-centered is-max-desktop">
            <div class="column is-four-fifths has-text-justified">
                <h2 class="title is-4 has-text-centered">Posterior sampling</h2>
                <p>
                    With the very same model, you can perform exact posterior sampling. Take the
                    $\varphi^4$ model. Consider the system with energy $E$ under a null magnetic
                    field and
                    assume a field configuration
                    can be sampled with coefficients $\alpha$ and $\beta$. It yields the prior
                    distribution.
                    Now, consider the same
                    system under a magnetic field $h$, its energy becomes $E_r=E + (h, \varphi)$.
                    By simply shifting the field configuration during generation process:
                    $\varphi_r=\varphi
                    +
                    \frac{\alpha^2}{\beta} h$,
                    you can sample from the posterior without <b>retraining or any kind of
                        approximation</b>.
                    Training data was generated with Hamiltonian MCMC. The figure below shows that stochastic
                    interpolants generate data that reproduce the magnetisation statistic $M(\varphi) = \frac1N
                    \sum\limits_{a} \varphi(a)$ for $h = 0$ and $h \ne 0$ with a single model.
                </p>
                <div class="has-text-centered"><b>Model: $\varphi^4$</b></div>
                <div class="container" , style="margin-bottom:25px">

                    <div class="imgContainer">
                        <img src="static/images/phi4-post-1.png" height="80%" width="80%" style="float:left">
                    </div>
                </div>
                <hr
                    style="border:1px solid black; background: black; border: none; margin-left: 350px; margin-right: 350px; margin-bottom:-10px; margin-top: -10px;">

                <div class="container has-text-centered" , style="margin-top: 15px">
                    <div class="imgContainer">
                        <img src="static/images/original.jpg" height="23.5%" width="23.5%" ,
                            style="position: relative; top: 50%; transform: translateY(12px);">
                        <p style="margin: 13px 0px 0px 0px;">Original image</p>
                    </div>
                    <div class="imgContainer">
                        <video poster="" id="steve" autoplay controls muted loop playsinline height="50%" width="50%"
                            style="position: relative; right: 180px; transform: translateY(5px)">
                            <source src="static/videos/phi4.mp4" type="video/mp4">
                        </video>
                        <p style="position: relative; right: 180px;">Block masking</p>
                    </div>
                </div>
            </div>
        </div>
        <!-- Posterior sampling. -->
    </section>

    <hr style="border:1px solid black; background: black; border: none; margin-left: 350px; margin-right: 350px">
    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <h1 class="title is-2 has-text-centered"> Theoretical Framework</h1>
            <p>
                Interpolation involves two multivariates random variables $x_0$ $\perp$ $x_1$ and a
                parametrization by two scalars $\alpha$ and $\beta$ via the
                very simple relation $I_{\alpha, \beta}(x_0,x_1) = \alpha x_0 + \beta x_1$, where $x_0 \sim
                \rho_0$, $x_1 \sim \rho_1$, and $\alpha, \beta \in [0, 1]$. $\rho_0$ and $\rho_1$ are
                respectively noise and data
                distributions, living in $\mathbb{R}^n$. In most of the usual case, $\alpha=\alpha(t) \equiv 1-t$,
                $\beta=\beta(t)=1-\alpha(t)\equiv t$. Interpolants can be formulated as
                flow matching, whose drift $(v_\alpha)_{\alpha \in [0, 1]}$ is learned with the Flow matching loss.
                By their nature of scalar parameters, they acts uniformally on every entries, letting little
                flexibility in the diffusion process.
                <br>
                Now, define $I_{\alpha, \beta}(x_0, x_1) = \alpha \odot x_0 + \beta \odot x_1$ with vector
                $\alpha \in [0,1]^n$ and $\beta\equiv 1-\alpha \in [0,1]^n$. It significantly increases the
                size of input space, but pixel-wise inpainting becomes a trivial task.
            </p>
            <p style="background-color: #f0f8ff; padding: 1em; border-left: 4px solid #2970CC; margin: 1em 0;">
                <b>Vanilla Flow matching loss:</b>
                <br><br>
                $$
                \mathcal{L}_{FM}(\hat v)=\mathbb{E}_{x_0 \sim \rho_0, x_1 \sim \rho_1, t \in
                \mathcal{U}([0,
                1])}[||\dot{\alpha}_t x_0 + \dot{\beta}_tx_1 - \hat v_t(\alpha_t x_0 + \beta_t x_1)||^2]
                $$
                $$
                \quad \quad \, \, \,\,= \mathbb{E}_{x_0 \sim \rho_0, x_1 \sim \rho_1, t \in
                \mathcal{U}[0,1]}[||x_1-x_0 - \hat v_t((1-t)x_0 + tx_1)||^2]
                $$
                <b>Multitask Stochastic Interpolants Loss:</b>
                <br><br>
                $$
                \mathcal{L}_{MTSI}(\hat v)=\mathbb{E}_{x_0 \sim \rho_0, x_1 \sim \rho_1, t \in
                \mathcal{U}([0,1]^n)}[||\dot{\alpha}_t \odot x_0 + (1-\dot{\alpha}_t) \odot x_1 - \hat v_t(\alpha_t
                \odot
                x_0 + (1-\alpha_t) \odot x_1)||^2]
                $$
            </p>
            <h2 class="title is-4">Inpainting</h2>
            <p>
                If $\alpha$ is parameterized by a
                vector time $t \in [0, 1]^n$ with for example $\alpha=1-t$, then you can:
            </p>
            <ul>
                <li>Generate entry $i \in \llbracket 1, n\rrbracket$ by letting $t_i$ go from $0$ to $1$.</li>
                <li>Keep entry $j$ fixed by imposing $t_j=1$ during all the generation process </li>
            </ul>
            <br>
            <h2 class="title is-4">Posterior sampling</h2>
            Consider the situation where $\rho_1$ is a prior, and let $r(x)=\langle Ax, x \rangle + \langle b, x\rangle$
            a log-likelihood. The posterior then takes the natural form: $\rho_1^r\propto \rho_1 e^{r}$ and an
            interpolant is: $I_r(x_0, x_1^r)=\alpha \odot x_0 + (1 - \alpha) \odot x_1^r$, with $x_0 \perp x_1^r$ and
            $x_0 \sim N(0, \text{Id})$, $x_1^r \sim \rho_1^r$.
            <p style="background-color: #f0f8ff; padding: 1em; border-left: 4px solid #2970CC; margin: 1em 0;">
                <b>Theorem:</b>
                <br><br>
                Let
                $$
                \eta_0(\alpha,\beta,x) = \mathbb{E}\big[x_0| I(\alpha,\beta) = x\big], \qquad \eta_1(\alpha,\beta,x) =
                \mathbb{E}\big[x_1| I(\alpha,\beta) = x\big],
                $$
                be the drifts associated with the interpolant and
                $$
                \eta^r_0(\alpha,\beta,x) = \mathbb{E}\big[x_0| I_r(\alpha,\beta) = x\big], \qquad
                \eta^r_1(\alpha,\beta,x) =
                \mathbb{E}\big[x_1^r| I_r(\alpha,\beta) = x\big],
                $$
                be the drifts associated with the interpolant. If $\alpha$ and $\beta$ are
                invertible, then
                $$
                \eta^r_0(\alpha,\beta,x) = \alpha^{-1} \beta \beta_r^{-1} \alpha_r \eta_0(\alpha_r,\beta_r,x_r) +
                \alpha^{-1} (x - \beta \beta_r^{-1} x_r), \quad
                \eta^r_1(\alpha,\beta,x) =\eta_1(\alpha_r,\beta_r,x_r)
                $$
                as long as we can find a pair $(\alpha_r,\beta_r)$ that satisfies
                $$
                \beta_r^T \alpha_r^{-T} \alpha^{-1}_r \beta_r = \beta^T\alpha^{-T}\alpha^{-1} \beta - A
                $$
                and $x_r$ is given by
                $$
                x_r = \alpha_r \alpha_r^T \beta_r^{-T}\left( \beta^T \alpha^{-T} \alpha^{-1} x + b\right).
                $$
            </p>
            <p style="background-color: #f0f8ff; padding: 1em; border-left: 4px solid #2970CC; margin: 1em 0;">
                <b>Theorem [Posterior drift]:</b>
                <br>
                Assume $\beta = 1 - \alpha$ with $\alpha$ diagonal and invertible, and let
                $$
                \eta^r(\alpha,x) = \mathbb{E}\big[x_0-x_1|\alpha x_0 + (1-\alpha) x_1^r = x \big] = \eta_0^r(\alpha,
                1-\alpha,
                x) - \eta_1^r(\alpha, 1-\alpha, x)
                $$
                where the drifts $\eta_0^r$ and $\eta_1^r$ are defined above and
                , respectively. Assume also that $A$ is diagonal, non-positive definite, and
                invertible. Then, $\beta_r = 1 -\alpha_r$ and $\eta^r$ can be expressed as:
                $$
                \eta^r(\alpha, x) = \alpha^{-1}\alpha_r\eta(\alpha_r, x_r) + \alpha^{-1}(x - x^r),
                $$
                where $\eta(\alpha, x)$ is the drift of the prior, and $\alpha_r$ and $x_r$ are given by
                $$
                \alpha_r = \alpha\frac{ \sqrt{1 - 2\alpha + \alpha^2(1 - A)} -\alpha }{1 - 2\alpha - \alpha^2A}
                $$
                $$
                x_r = \frac{\alpha_r^2(1-\alpha)}{\alpha^2 (1-\alpha_r)} \, x + \frac{\alpha_r^2}{(1-\alpha)}\, b,
                $$
            </p>
        </div>
    </div>

    <section class=" section" id="BibTeX">
        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">BibTeX</h2>
                <pre><code>@article{negrel2025multitasklearningstochasticinterpolants,
        author    = {Negrel, Hugo and Coeurdoux, Florentin and Albergo, Michael and Vanden-Eijnden, Eric},
        title     = {Multitask Learning with Stochastic Interpolants},
        journal   = {NeurIPS2025},
        year      = {2025},
        url       = {https://arxiv.org/abs/2508.04605}
                }</code></pre>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            This webpage is adapted from <a
                                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>